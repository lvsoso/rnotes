(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{493:function(v,_,t){"use strict";t.r(_);var l=t(8),a=Object(l.a)({},(function(){var v=this,_=v._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[_("h3",{attrs:{id:"推理过程"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#推理过程"}},[v._v("#")]),v._v(" 推理过程")]),v._v(" "),_("p",[v._v("1.输入处理")]),v._v(" "),_("ul",[_("li",[v._v("分词：拆分token")]),v._v(" "),_("li",[v._v("向量嵌入：查找对应的向量。token本身有对应的向量 N"),_("em",[v._v("12288，“位置（输入中的位置）”也有其对应的向量 N")]),v._v("12288")]),v._v(" "),_("li",[v._v("词向量+位置向量：两个矩阵相加 产生一个输出矩阵")])]),v._v(" "),_("p",[v._v("2.编码器处理")]),v._v(" "),_("p",[v._v("96 层解码器（自注意力子层+前馈神经网络子层）")]),v._v(" "),_("p",[_("strong",[v._v("自注意力子层")])]),v._v(" "),_("p",[v._v("解码器多头自回归自注意力机制；")]),v._v(" "),_("p",[v._v("注意力：输入序列与输出序列之间的关系、依赖；\n自注意力：没有严格的输出序列，关注输入序列；")]),v._v(" "),_("p",[v._v("自回归/单向/因果：自注意力机制")]),v._v(" "),_("p",[v._v("多头：分成多头，分别取不同的语义表示")]),v._v(" "),_("p",[v._v("每个自注意力头的q问题、k索引、v答案矩阵：12288 * 128")]),v._v(" "),_("p",[v._v("自注意力头计算： n * 12288 dot 12288 * 128  = n * 128")]),v._v(" "),_("p",[v._v("qk转置/点积计算： Q(n * 128) dot K(128 * n) = n*n 权重矩阵")]),v._v(" "),_("p",[v._v("缩放：除以sqrt(12288)")]),v._v(" "),_("p",[v._v("掩码矩阵相乘： 下三角矩阵 1\\")]),v._v(" "),_("p",[v._v("softmax：")]),v._v(" "),_("p",[v._v("乘以 V ：n * n dot n * 128 = n * 128")]),v._v(" "),_("p",[v._v("多头向量(96 份输出)拼接：n * 128 * 96 = n * 12288")]),v._v(" "),_("p",[v._v("全连接线性层：n * 12288 * 12288 * 12288 = n * 12288")]),v._v(" "),_("p",[v._v("激活函数：非线性化、GRLU、RELU")]),v._v(" "),_("p",[v._v("残差链接：原始输入+当前输出")]),v._v(" "),_("p",[v._v("归一化层：均值为0，标准差为1")]),v._v(" "),_("p",[_("strong",[v._v("前馈神经网络子层")])]),v._v(" "),_("p",[v._v("全连接扩张线性层： n * 12288 * (12288 * 4 * 12288) 扩张4倍")]),v._v(" "),_("p",[v._v("激活函数：非线性")]),v._v(" "),_("p",[v._v("全连接收缩线性层： ？ * 4 * 12288 * 12288 = n * 12288")]),v._v(" "),_("p",[v._v("残差连接：")]),v._v(" "),_("p",[v._v("归一化层：")]),v._v(" "),_("ol",{attrs:{start:"3"}},[_("li",[v._v("处理输出")])]),v._v(" "),_("p",[v._v("线性层：n * 12288 * (12288 * 50257) = n * 50257")]),v._v(" "),_("p",[v._v("softmax：")]),v._v(" "),_("p",[v._v("采样策略处理：确定下一个字")]),v._v(" "),_("p",[v._v("迭代处理：")]),v._v(" "),_("h3",{attrs:{id:"参数"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#参数"}},[v._v("#")]),v._v(" 参数")]),v._v(" "),_("p",[v._v("输入词处理：\n词向量参数矩阵 50257 * 122288\n位置向量参数矩阵 2048 * 12288")]),v._v(" "),_("p",[v._v("自注意力矩阵：( 12288 * 128 + 128 )* 3 * 96 + 12288*12288 + 12288\n前馈神经网络子层： (12288 * 12288 * 4 + 12288 )+ （4 * 12288 * 12288 + 12288）")]),v._v(" "),_("p",[v._v("解码器层（自注意力矩阵+前馈神经网络）* 96")]),v._v(" "),_("p",[v._v("输出： 12288 * 50257 + 50257")]),v._v(" "),_("h3",{attrs:{id:"训练"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#训练"}},[v._v("#")]),v._v(" 训练")]),v._v(" "),_("ul",[_("li",[v._v("初始化参数为随机值")]),v._v(" "),_("li",[v._v("计算输出与真实数据的差距（损失和梯度）")]),v._v(" "),_("li",[v._v("根据损失逐步调整权重参数")])]),v._v(" "),_("p",[v._v("真实数据使用 one-hot编码矩阵")]),v._v(" "),_("p",[v._v("交叉熵损失：预测结果取对数-与目标矩阵逐步位置相乘-求和-取反-除序列长度-训练集求和取平均-平均损失值")]),v._v(" "),_("h4",{attrs:{id:"梯度计算"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#梯度计算"}},[v._v("#")]),v._v(" 梯度计算")]),v._v(" "),_("p",[v._v("起始梯度：参数调整反向传播的起点，可以用目标矩阵减去预测结果矩阵")]),v._v(" "),_("p",[v._v("梯度反向传播：链式法则")]),v._v(" "),_("p",[v._v("参数矩阵梯度：输入矩阵转置乘输出矩阵梯度")]),v._v(" "),_("p",[v._v("偏置向量梯度：等于逐列求和输出矩阵梯度")]),v._v(" "),_("p",[v._v("输入矩阵梯度：等于输出矩阵梯度乘参数矩阵的梯度")]),v._v(" "),_("h4",{attrs:{id:"学习率"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#学习率"}},[v._v("#")]),v._v(" 学习率")]),v._v(" "),_("ul",[_("li",[v._v("Adam： 计算均值和方差，动态调整学习率")]),v._v(" "),_("li",[v._v("随机梯度下降")]),v._v(" "),_("li",[v._v("小批量梯度下降")])]),v._v(" "),_("p",[v._v("GPT 使用 Adam, 初始值比较小")]),v._v(" "),_("p",[v._v("新的参数矩阵：参数矩阵-梯度矩阵 * 学习率\n偏置向量：偏置向量-梯度向量 * 学习率")]),v._v(" "),_("h4",{attrs:{id:"gpt-训练集"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#gpt-训练集"}},[v._v("#")]),v._v(" GPT 训练集")]),v._v(" "),_("ul",[_("li",[v._v("训练序列：2048 token;")]),v._v(" "),_("li",[v._v("训练批次：多个训练序列，3.2M tokens;")])]),v._v(" "),_("p",[v._v("每一个训练批次中，每一个输入序列都会计算一遍梯度，最后进行求和取平均得到平均梯度值，最后进行统一的参数调整；")]),v._v(" "),_("h3",{attrs:{id:"微调"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#微调"}},[v._v("#")]),v._v(" 微调")]),v._v(" "),_("p",[v._v("分类：无监督、自监督、半监督、监督、强化、迁移、联邦\n标签：模型预测的目标变量，标注数据\nfine tuning：有目的性的调整（参数、作用位置、影响）")]),v._v(" "),_("p",[v._v("FT分类：")]),v._v(" "),_("ul",[_("li",[v._v("SFT：少量标签、会使用标签数据走训练、会走反向传播更新参数")]),v._v(" "),_("li",[v._v("Prompt：提示词微调，对输入序列嵌入额外的token，可能更新参数？不一定走反向传播？")]),v._v(" "),_("li",[v._v("Prefix：前缀微调，嵌入自有的向量/虚的token？，会参与训练，会修改虚token部分涉及的参数")]),v._v(" "),_("li",[v._v("LORA：低秩矩阵适应微调，用一个低秩矩阵叠加进参数矩阵")]),v._v(" "),_("li",[v._v("AT：adapter tuning，插件微调，插入小网络/矩阵，不影响原有参数")]),v._v(" "),_("li",[v._v("RLHF：基于人类反馈的强化学习，建立奖励模型、引导，参与正向传播和反向传播")])]),v._v(" "),_("p",[v._v("chatGPT: 300B token 迭代一次, RLHF(SFT,RM,PPO)")]),v._v(" "),_("h3",{attrs:{id:"空间占用和算力"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#空间占用和算力"}},[v._v("#")]),v._v(" 空间占用和算力")]),v._v(" "),_("p",[v._v("模型空间占用：\n175000000000 * 16 / 8 / 1024 / 1024 / 1024 / 1024 ～ 326.36G")]),v._v(" "),_("p",[v._v("3.14* 10 ^ 23 FLOPS\n300B Token")]),v._v(" "),_("h3",{attrs:{id:"门槛"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#门槛"}},[v._v("#")]),v._v(" 门槛：")]),v._v(" "),_("ol",[_("li",[v._v("参数")])]),v._v(" "),_("ul",[_("li",[v._v("词汇表")]),v._v(" "),_("li",[v._v("词向量宽度/隐藏层宽度")]),v._v(" "),_("li",[v._v("解码器层数")]),v._v(" "),_("li",[v._v("自注意力头数")]),v._v(" "),_("li",[v._v("参数精度")])]),v._v(" "),_("ol",{attrs:{start:"2"}},[_("li",[v._v("算力")])]),v._v(" "),_("ul",[_("li",[v._v("芯片")]),v._v(" "),_("li",[v._v("加速卡")]),v._v(" "),_("li",[v._v("服务器")]),v._v(" "),_("li",[v._v("网络")]),v._v(" "),_("li",[v._v("环境")])]),v._v(" "),_("ol",{attrs:{start:"3"}},[_("li",[v._v("训练数据")])]),v._v(" "),_("ul",[_("li",[v._v("原始数据")]),v._v(" "),_("li",[v._v("总Token数")]),v._v(" "),_("li",[v._v("训练数据量")])]),v._v(" "),_("h3",{attrs:{id:"与大脑对比"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#与大脑对比"}},[v._v("#")]),v._v(" 与大脑对比")]),v._v(" "),_("ul",[_("li",[v._v("参数数量：神经元连接数")]),v._v(" "),_("li",[v._v("权重：强弱")]),v._v(" "),_("li",[v._v("规模：人脑>>大模型")]),v._v(" "),_("li",[v._v("过程：人脑突触重塑，不会反向传播，大模型推训分离")]),v._v(" "),_("li",[v._v("功耗：人脑<<大模型")]),v._v(" "),_("li",[v._v("结构：相差大")])]),v._v(" "),_("h3",{attrs:{id:"编码器对比解码器"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#编码器对比解码器"}},[v._v("#")]),v._v(" 编码器对比解码器")]),v._v(" "),_("ul",[_("li",[v._v("编码器的输出根据任务而定，解码器输出就是下一个字的概率矩阵；")]),v._v(" "),_("li",[v._v("编码器通过MLM（随机掩码学习）NSP（下一句预测）等方式训练，解码器训练下一个字的预测和生成；")]),v._v(" "),_("li",[v._v("编码器的训练方法与推理场景不完全匹配、导致参数不合适；")]),v._v(" "),_("li",[v._v("编码器吸收多，输出少；")]),v._v(" "),_("li",[v._v("解码器更基本；")]),v._v(" "),_("li",[v._v("大参数、大算力、大数据支持；")])]),v._v(" "),_("h3",{attrs:{id:"自注意力溯源"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#自注意力溯源"}},[v._v("#")]),v._v(" 自注意力溯源")]),v._v(" "),_("ul",[_("li",[v._v("神经网络\n"),_("ul",[_("li",[v._v("深度神经网络\n"),_("ul",[_("li",[v._v("CNN（卷积神经网络）")]),v._v(" "),_("li",[v._v("RNN（循环神经网络）\n"),_("ul",[_("li",[v._v("LSTM、CRU")]),v._v(" "),_("li",[v._v("StoS 序列到序列模型\n"),_("ul",[_("li",[v._v("编码器+解码器")])])])])]),v._v(" "),_("li",[v._v("注意力机制\n"),_("ul",[_("li",[v._v("Transformer\n"),_("ul",[_("li",[v._v("纯编码器（双向自注意力）")]),v._v(" "),_("li",[v._v("纯解码器（单向自注意力）")])])])])])])])])])]),v._v(" "),_("h3",{attrs:{id:"大模型的可解释性"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#大模型的可解释性"}},[v._v("#")]),v._v(" 大模型的可解释性？")]),v._v(" "),_("h4",{attrs:{id:"人类思考的三种模式"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#人类思考的三种模式"}},[v._v("#")]),v._v(" 人类思考的三种模式")]),v._v(" "),_("ul",[_("li",[v._v("逻辑思考/推理\n"),_("ul",[_("li",[v._v("演绎")]),v._v(" "),_("li",[v._v("归纳")])])]),v._v(" "),_("li",[v._v("概率/统计")]),v._v(" "),_("li",[v._v("相关")]),v._v(" "),_("li",[v._v("直觉")])]),v._v(" "),_("h4",{attrs:{id:"人工智能的两种范式"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#人工智能的两种范式"}},[v._v("#")]),v._v(" 人工智能的两种范式")]),v._v(" "),_("ul",[_("li",[v._v("符号主义？（形式逻辑）\n"),_("ul",[_("li",[v._v("预先设定的规则")]),v._v(" "),_("li",[v._v("确定的、可解释的")])])]),v._v(" "),_("li",[v._v("连接主义？（神经网络）\n"),_("ul",[_("li",[v._v("自主学习到的参数（规则、逻辑）")]),v._v(" "),_("li",[v._v("随机的、不可解释的")])])])]),v._v(" "),_("h4",{attrs:{id:"大模型的随机性"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#大模型的随机性"}},[v._v("#")]),v._v(" 大模型的随机性")]),v._v(" "),_("ul",[_("li",[v._v("推理\n"),_("ul",[_("li",[v._v("输出层（随机确定下一个token）\n"),_("ul",[_("li",[_("blockquote",[_("p",[v._v("好处：多样性、鲁棒性、有助于创意式生成")])])]),v._v(" "),_("li",[_("blockquote",[_("p",[v._v("坏处：没有 确定性/准确性、幂等性、可管理性，不利于创意式生成")])])])])])])]),v._v(" "),_("li",[v._v("训练\n"),_("ul",[_("li",[v._v("参数随机初始化")]),v._v(" "),_("li",[v._v("随机打乱训练数据/随机梯度下降")]),v._v(" "),_("li",[v._v("Dropout（随机关闭神经元）")])])])]),v._v(" "),_("h4",{attrs:{id:"大模型的参数不可解读性"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#大模型的参数不可解读性"}},[v._v("#")]),v._v(" 大模型的参数不可解读性")]),v._v(" "),_("ul",[_("li",[v._v("结构无区别")]),v._v(" "),_("li",[v._v("随机初始化")]),v._v(" "),_("li",[v._v("数据量大（参数）")]),v._v(" "),_("li",[v._v("无法参照现有人类知识体系解读")])]),v._v(" "),_("p",[v._v("好处：智能？")]),v._v(" "),_("p",[v._v("坏处：难以管理、压缩、裁剪、定向加强/减弱")]),v._v(" "),_("h3",{attrs:{id:"未来"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#未来"}},[v._v("#")]),v._v(" 未来")]),v._v(" "),_("ul",[_("li",[v._v("插件\n"),_("ul",[_("li",[v._v("计算")]),v._v(" "),_("li",[v._v("搜索")]),v._v(" "),_("li",[v._v("代码")])])]),v._v(" "),_("li",[v._v("多模态")]),v._v(" "),_("li",[v._v("参数规模")]),v._v(" "),_("li",[v._v("多模型\n"),_("ul",[_("li",[v._v("MoE")])])]),v._v(" "),_("li",[v._v("推训一体（长期记忆）")]),v._v(" "),_("li",[v._v("分布式")]),v._v(" "),_("li",[v._v("个性化")])])])}),[],!1,null,null,null);_.default=a.exports}}]);