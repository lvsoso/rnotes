# 大模型

## AI Infra

分层
- MaaS：产品
- PaaS：平台
- IaaS：基础设施

## AI Model
- 数据
- 算法
- 算力

硬件和软件？

## LLM as Controller

### HuggingGPT
- 任务规划
- 模型选择
- 任务执行
- 生成结果

### Toolformer

使用多种外部工具。

### AutoGPT

input->plan->assign->collect->output

### NexusGPT

每个Agent实际上可以认为是一个人，每个agent擅长不同的事情，那么一个庞大的组织架构可以通过雇佣擅长各种能力的Agent来维持组织的正常运转。

### Generative Agents

生成式智能体。

https://zhuanlan.zhihu.com/p/640725385?utm_id=0



###  批量处理
“将多个句子拼接到一块儿作为输入” 不等于 “批量处理”。

**批量处理** 一般说的是并行批量。

将多个相互独立的问题作为输入进行批量并行处理。

- 使用 bacth tokenizer，截断或者填充；
- 构造prompt序列；
- 生成；
- 需要等待所有的结果生成完毕了；

加快速度？生成的结果长度不会相差太大。


### 大模型推理加速

[资料](https://pan.baidu.com/s/1hKlrHYe0BviQUopY3hUJ1g?pwd=3mv8)

#### 基本结构
- Layernorm
- Attention
- Silu
- MatMul
- Rotary Embedding
- KV Cache

大部分计算量在矩阵乘法。

#### 推理
- Prefill -> 大部分时间
- Decoding

子过程构成：
- Tokenize 将文本转换为向量 CPU
- Computing 模型推理 GPU
- Detokenize 将向量转为文本 CPU
- Sampling 依据推理结果进行采样

优化：
- 流水线策略
- 高效算子
- 动态批处理：merge step 混合多个用户的，Decoding Attention？
- VM Allocator： 显存的分配优化（最大分配、页框、VM Allocator 预测的方式）
- 显存不够切到内存？通讯带宽太低、切换操作时间长、阻塞CPU？
- KV Cache 量化：每个用户都有自己的kvcache，占用大部分的显存空间、限制系统的并发请求数，进行量化压缩减少占用。 
- 矩阵乘法量化：服务端 int8 加速运算（计算密集），INT4 Weight Only 量化适用访存密集型的矩阵乘法

页框无上限？不好估计用户容量？swap机制？

INT4 Weight Only 在batch大的情况下，  解量化、计算都占很多时间 ？终端多使用int4

FP8： https://zhuanlan.zhihu.com/p/574825662


#### 硬件
- 根据模型结构可以预估所有算子的计算量和访存需求，进而预估推理性能；

  

#### Benchmark
- Thougtput（吞吐量）：同一时间多个输入，测量prefill、decodeing
- First Token Latency（首字延迟）：多长时间才能给出响应，prefill阶段
- Latency（延迟）：每个decoding时间
- QPS（每秒请求数量）： K / 完成K个请求的时间
  

影响因素：
- batch
- input len

