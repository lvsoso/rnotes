### 推理过程

1.输入处理

- 分词：拆分token
- 向量嵌入：查找对应的向量。token本身有对应的向量 N*12288，“位置（输入中的位置）”也有其对应的向量 N*12288
- 词向量+位置向量：两个矩阵相加 产生一个输出矩阵


2.编码器处理

96 层解码器（自注意力子层+前馈神经网络子层）


**自注意力子层**

解码器多头自回归自注意力机制；

注意力：输入序列与输出序列之间的关系、依赖；
自注意力：没有严格的输出序列，关注输入序列；

自回归/单向/因果：自注意力机制

多头：分成多头，分别取不同的语义表示

每个自注意力头的q问题、k索引、v答案矩阵：12288 * 128

自注意力头计算： n * 12288 dot 12288 * 128  = n * 128

qk转置/点积计算： Q(n * 128) dot K(128 * n) = n*n 权重矩阵

缩放：除以sqrt(12288)

掩码矩阵相乘： 下三角矩阵 1\

softmax：

乘以 V ：n * n dot n * 128 = n * 128

多头向量(96 份输出)拼接：n * 128 * 96 = n * 12288

全连接线性层：n * 12288 * 12288 * 12288 = n * 12288

激活函数：非线性化、GRLU、RELU

残差链接：原始输入+当前输出

归一化层：均值为0，标准差为1


**前馈神经网络子层**

全连接扩张线性层： n * 12288 * (12288 * 4 * 12288) 扩张4倍

激活函数：非线性

全连接收缩线性层： ？ * 4 * 12288 * 12288 = n * 12288

残差连接：

归一化层：


3. 处理输出

线性层：n * 12288 * (12288 * 50257) = n * 50257

softmax：

采样策略处理：确定下一个字

迭代处理：

### 参数

输入词处理：
词向量参数矩阵 50257 * 122288 
位置向量参数矩阵 2048 * 12288

自注意力矩阵：( 12288 * 128 + 128 )* 3 * 96 + 12288*12288 + 12288
前馈神经网络子层： (12288 * 12288 * 4 + 12288 )+ （4 * 12288 * 12288 + 12288）


解码器层（自注意力矩阵+前馈神经网络）* 96 

输出： 12288 * 50257 + 50257

### 训练
- 初始化参数为随机值
- 计算输出与真实数据的差距（损失和梯度）
- 根据损失逐步调整权重参数


真实数据使用 one-hot编码矩阵

交叉熵损失：预测结果取对数-与目标矩阵逐步位置相乘-求和-取反-除序列长度-训练集求和取平均-平均损失值

#### 梯度计算

起始梯度：参数调整反向传播的起点，可以用目标矩阵减去预测结果矩阵

梯度反向传播：链式法则

参数矩阵梯度：输入矩阵转置乘输出矩阵梯度

偏置向量梯度：等于逐列求和输出矩阵梯度

输入矩阵梯度：等于输出矩阵梯度乘参数矩阵的梯度

#### 学习率

- Adam： 计算均值和方差，动态调整学习率
- 随机梯度下降
- 小批量梯度下降


GPT 使用 Adam, 初始值比较小


新的参数矩阵：参数矩阵-梯度矩阵 * 学习率
偏置向量：偏置向量-梯度向量 * 学习率




#### GPT 训练集

- 训练序列：2048 token;
- 训练批次：多个训练序列，3.2M tokens;

每一个训练批次中，每一个输入序列都会计算一遍梯度，最后进行求和取平均得到平均梯度值，最后进行统一的参数调整；


### 微调

分类：无监督、自监督、半监督、监督、强化、迁移、联邦
标签：模型预测的目标变量，标注数据
fine tuning：有目的性的调整（参数、作用位置、影响）

FT分类：
- SFT：少量标签、会使用标签数据走训练、会走反向传播更新参数
- Prompt：提示词微调，对输入序列嵌入额外的token，可能更新参数？不一定走反向传播？
- Prefix：前缀微调，嵌入自有的向量/虚的token？，会参与训练，会修改虚token部分涉及的参数
- LORA：低秩矩阵适应微调，用一个低秩矩阵叠加进参数矩阵
- AT：adapter tuning，插件微调，插入小网络/矩阵，不影响原有参数
- RLHF：基于人类反馈的强化学习，建立奖励模型、引导，参与正向传播和反向传播


chatGPT: 300B token 迭代一次, RLHF(SFT,RM,PPO)

### 空间占用和算力

模型空间占用：
175000000000 * 16 / 8 / 1024 / 1024 / 1024 / 1024 ～ 326.36G

3.14* 10 ^ 23 FLOPS
300B Token


### 门槛：

1. 参数
- 词汇表
- 词向量宽度/隐藏层宽度
- 解码器层数
- 自注意力头数
- 参数精度

2. 算力
- 芯片
- 加速卡
- 服务器
- 网络
- 环境

3. 训练数据
- 原始数据
- 总Token数
- 训练数据量

